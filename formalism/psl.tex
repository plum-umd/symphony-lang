\documentclass[10pt]{article}

\usepackage{times}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathpartir}
\usepackage{color}
\usepackage{xspace}
\usepackage[override]{cmtt}

%%%%% DEFS %%%%%%

\newcommand{\rulelab}[1]{{\small \textsc{#1}}}
\newcommand{\kw}[1]{\ensuremath{\mathtt{#1}}}

% Types
\newcommand{\tnat}{\ensuremath{\mathtt{nat}}}
\newcommand{\tfun}[3]{\ensuremath{{#1} ~^{#3}\!\!\rightarrow {#2}}}
\newcommand{\tpair}[2]{\ensuremath{{#1} \times {#2}}}
%\newcommand{\tsum}[4]{\ensuremath{{#1} +^{#3}_{#4} {#2}}}
\newcommand{\tsum}[3]{\ensuremath{{#1} +^{#3} {#2}}}
\newcommand{\trec}[2]{\ensuremath{\mu {#1}.{#2}}}
% \newcommand{\ssec}{\ensuremath{\mathtt{\cdot}}}
% \newcommand{\isec}{\ensuremath{\mathtt{pmap}}}
% \newcommand{\sshare}[1]{\ensuremath{\mathtt{shr}~{#1}}}
% \newcommand{\sectyp}[3]{\ensuremath{{#1} \{~{#2}:{#3}~\}}}
  
% Terms
\newcommand{\ebinop}[2]{\ensuremath{{#1}~\oplus~{#2}}}
\newcommand{\elet}[3]{\ensuremath{\kw{let}~#1\, =\, #2~\kw{in}\;{#3}}}
\newcommand{\epar}[2]{\ensuremath{\kw{par}~{#1}~{#2}}}
\newcommand{\esec}[2]{\ensuremath{\kw{sec}~{#1}~{#2}}}
\newcommand{\ereveal}[3]{\ensuremath{\kw{reveal}\{{#1}\!:\!{#2}\}~{#3}}}
\newcommand{\econd}[3]{\ensuremath{\kw{case}~{#1}~x.{#2} \diamond {#3}}}
\newcommand{\emux}[3]{\ensuremath{{#1}~\kw{?}~{#2} \diamond {#3}}}
\newcommand{\eshare}[4]{\ensuremath{\kw{share}\{{#1}\!:\!{#2}\rightarrow{#3}\}~{#4}}}
\newcommand{\einj}[2]{\ensuremath{\kw{inj}_{#1}~{#2}}}
\newcommand{\eread}{\ensuremath{\kw{read}}}
\newcommand{\ewrite}[1]{\ensuremath{\kw{write}~{#1}}}
\newcommand{\epair}[2]{\ensuremath{\langle {#1}, {#2} \rangle}}
\newcommand{\eproj}[2]{\ensuremath{\kw{\#}}_{#1}~{#2}}
\newcommand{\elam}[2]{\ensuremath{\lambda {#1}.{#2}}}
\newcommand{\eapp}[2]{\ensuremath{{#1}~{#2}}}
\newcommand{\efix}[3]{\ensuremath{\kw{fix}~{#1}.\elam{#2}{#3}}}
\newcommand{\efold}[2]{\ensuremath{\kw{fold}_{#1}~{#2}}}
\newcommand{\eunfold}[1]{\ensuremath{\kw{unfold}~{#1}}}
\newcommand{\vshare}[3]{\ensuremath{\{{#3}\}^{#1}_{#2}}}
\newcommand{\vloc}[2]{\ensuremath{{#1}\kw{@}{#2}}}
\newcommand{\vclos}[2]{\ensuremath{\mathbf{clos}~({#1},{#2})}}
\newcommand{\vcrash}{\ensuremath{\bullet}}

% Judgments
\newcommand{\hastyp}[4]{\ensuremath{{#1} \vdash_{#2} {#3} : {#4}}}
%\newcommand{\canshare}[1]{\ensuremath{\text{isFlat } {#1}}}
\newcommand{\subtype}{\ensuremath{<:}}
\newcommand{\issub}[2]{{#1} \subtype {#2}}
\newcommand{\eval}[4]{\ensuremath{\config{#1}{#3} \longrightarrow_{#2} {#4}}}
\newcommand{\seval}[5]{\ensuremath{\config{#1}{#3} \rightsquigarrow_{#2} \config{#4}{#5}}}

% Aux
\newcommand{\env}{\ensuremath{\sigma}}
\newcommand{\config}[2]{\ensuremath{\langle{#1},{#2}\rangle}}
\newcommand{\locof}[2]{\ensuremath{\mathit{loc}_{#1}~{#2}}}
\newcommand{\getat}[2]{\ensuremath{\mathit{on}_{#1}~{#2}}}
\newcommand{\ctxt}{\ensuremath{\mathcal{E}}}
\newcommand{\pctxt}{\ensuremath{\mathcal{P}}}
\newcommand{\hole}{\ensuremath{\circ}}

\newcommand{\lang}{\textsc{corePSL}\xspace}

%%%%%%%%%%

\newcommand{\mwh}[1]{\textcolor{red}{Mike: #1}}

\title{\lang: A core formalism for the PANTHEON Source Language}
\author{Michael Hicks and the PANTHEON Team}

\begin{document}

\maketitle

\section{Introduction}

This document defines \lang, a core calculus modeling PSL, the
\emph{PANTHEON Source Language} for programming secure multiparty
computations (MPCs). The \lang core calculus consists of the key features of PSL,
leaving out features that can be encoded in terms of the core. Indeed,
we expect the PANTHEON intermediate language (PIL) to look closer to
\lang, once it's fully developed.

\subsection{Overview}
\label{sec:overview}

PSL (and \lang) was inspired by the MPC programming language
Wysteria. Like Wysteria, an MPC is written as a single program to be
executed by multiple parties. This approach improves on the common
alternative design in which each party has a distinct MPC
program that must interact properly with the other parties'
programs. In such a design, the text of each program instance tends to
overlap significantly, encouraging a ``cut \& paste'' style of
development that can easily lead to mistakes.

\paragraph{Example: Millionaires}

Here is a version of the ``millionaires'' problem in \lang (which is
slightly more verbose than it would be in actual PSL, as discussed shortly):
\begin{verbatim}
1   let x = par{A} read in
2   let y = par{B} read in
3   let sx = par{A,C,D} share{yao:A -> C,D} x in
4   let sy = par{B,C,D} share{yao:B -> C,D} y in
5   let z = par{C,D,E} reveal{:E} par{C,D} sx < sy in
6   par{E} write z
\end{verbatim}
The program starts by reading an (integer) input at principal $A$'s
host, as specified by the \emph{par block} \texttt{par\{A\} read}. The
result, bound in \texttt{x}, is usable at \texttt{A} and not any of
the other participating hosts. In  
general, the construct $\epar{P}{e}$ runs the expression $e$ at the
hosts indicated in the set $P$, and the result will be available only
to those hosts. The PSL type system will reject programs that would
try to use inaccessible values, e.g., a variant of the above in
which $B$ tries to add $1$ to \texttt{x}.

After reading $B$'s input, too, the program \emph{encrypts} the two
inputs and shares them with parties $C$ and $D$, on lines 3 and
4. Both lines of code are in par blocks involving $C$ and $D$, with
line 3 also including $A$ (since it is involved in distributing its
share) and line 4 including $B$. The $\kw{share}$ construct specifies
the source and destination of the encrypted values, as well as the MPC
protocol that will be used to compute over them; in this case it is
\texttt{yao} (for garbled circuits). Variables \texttt{sx} and
\texttt{sy} are usable only at hosts $C$ and $D$.

Line 5 first performs the ``less than'' operation (\texttt{<}) on the
encrypted values at hosts $C$ and $D$. What actually happens depends
on the selected protocol. We can think of an operation on encrypted
values as constructing a garbled circuit where those values will be
fed as ``input wires'' to that circuit. The \texttt{reveal} construct
then forces the encrypted computation to actually take place, sending
the output as cleartext to party $E$. Doing so requires $C$, $D$, and
$E$ all to be executing in a par block together. Finally, at party
$E$, line 6 prints out the result.

The above program is a bit more verbose that it needs to be, due to
the extensive use of par blocks. While in \lang we are very explicit
about their use, in actual PSL a simple type inference pass can remove
infer all but the two par blocks on lines 1 and 2, meaning the program
would boil down to the following.
\begin{verbatim}
let x = par{A} read in
let y = par{B} read in
let sx = share{yao:A -> C,D} x in
let sy = share{yao:B -> C,D} y in
let z = reveal{:E} sx < sy in
write z
\end{verbatim}
Indeed, in PSL we can shorten it even further by using wire bundles,
which are not directly represented in \lang but can be encoded; we
show a 4-line version of millionaires in
Section~\ref{sec:wirebundles}.

\paragraph{Multiple programs}

Suppose we had used a language like EMP to write the above program
instead. We would essentially have to write five different programs,
each containing some fragment of the above PSL one, with some
programs significantly overlapping. For example, $A$'s program would
be something like the following (using the simple syntax introduced
so far)
\begin{verbatim}
let x = read in 
send-share{yao:C,D} x
\end{verbatim}
$B$'s program would be similar, with the only difference being details
regarding communication we have not shown. $C$'s program would be
\begin{verbatim}
let sx = recv-share{yao:C,D <- A} in
let sy = recv-share{yao:C,D <- B} in
let z = sx < sy in 
reveal-send{:E} z
\end{verbatim}
$D$'s program would be similar. Last, $E$'s program would be:
\begin{verbatim}
let z = reveal-recv{:C,D} in
write z
\end{verbatim}
Notice that the single \texttt{share} and \texttt{reveal} constructs
have been split into parts that do sending and receiving. Which
principal is running at any given time is implicit; it depends on who
is running the actual program.

There are three key downsides to separating programs. The first is
that the flow of control between them is harder to discern. In this
case, the program is small enough that we can mentally recover it; but
for a larger program involving many parties, doing so would be much
more difficult. 

The second downside is that there is more opportunity for mistakes,
whose likelihood is greater because of a program's reduced
understandability. In particular, because the \texttt{share} and
\texttt{reveal} constructs are separated into two parts, the
programmer may now introduce race conditions and deadlocks. For
example, $A$ might be sending to $B$ and waiting for it to acknowledge
receipt, but at the same time $B$ could be doing the same with $A$. The
single-program structure imposed by PSL assures this cannot happen. It
also prevents the mistake of party $A$ sending one kind of encrypted
value (e.g., a BGW share) while party $B$ expects to receive a
different kind (e.g., a GMW share)---the single \texttt{share}
construct ensures both agree. While not allowing primitive send and
receive commands reduces the programmer's options, this restriction is
akin to the situation with \texttt{goto} in structured programs as
argued by Dijkstra; the
potential harm of its use outweighs the small gain it affords in
performance.

The final downside is that it is harder to automatically analyze
multiple programs together compared to a single one, e.g.,
to do resource estimation. Such an analysis would boil down to
analyzing a multithreaded, message-passing program, which is hard to
do in the general case. By contrast, in PSL not only is there a single
program, but that program has a straightforward \emph{single threaded
  interpretation}. That is, we can faithfully simulate the program's
multi-party execution in a locally-hosted simulation. This works by
interpreting par blocks simply as a means to define and enforce access
to data based on the given principal set, not as a means to define
actually parallel and communicating computations. We are able to prove
that the multi-party interpretation of the program (basically, the
three programs we showed above) precisely bi-simulates the single
threaded one, and therefore reasoning about the latter will provide an
accurate view of what happens when running the former. \mwh{Could say
  something here about specializing behavior of divbyzero, etc.}

\paragraph{Wysteria}

PSL's design is inspired by that of Wysteria, which (for all the
reasons given above) also advocated
writing a single MPC program with a sound single-threaded
interpretation. PSL's use of par blocks is basically the same as
Wysteria's. To perform an actual encrypted computation, Wysteria
defined \emph{sec blocks} whose inputs were \emph{wires}; these
language features bundle together PSL's notions of secret sharing,
circuit construction (over shares), and computation of the final
result. For example, here is a variant of millionaire's in Wysteria
where $A$ and $B$ perform the computation themselves, rather than
involving $C$ and $D$.
\begin{verbatim}
1   let x = wire{A} par{A} read in
2   let y = wire{B} par{B} read in
3   let z = sec{A,B,E} wire{E} (x[A] < y[B]) in
4   par{E} write z[E]
\end{verbatim}
In this program, after $A$ and $B$ read their input values they stick
them in a wire annotated with their identity. On line 3, we enter a
sec block involving $A$, $B$, and $E$. Within, it compares the $A$'s
value and $B$'s value, and packages the result in a wire to $E$.
On line 4, $E$ outputs the result it received.  With
this program, we did not have to explicitly create encrypted values;
the sec block does this implicitly. It also implicitly reveals any
returned result to cleartext, here stored in a wire only accessible by
$E$. This is convenient.

However, it turns out that it is \emph{not possible in Wysteria to implement the
PSL program we started with}. While Wysteria does have a notion of
encrypted values that can live outside a sec block, it is not flexible
enough to allow such values to be communicated freely among different
compute parties. Relatedly, while one can interleave local and
multi-party computations in Wysteria, the use of sec blocks sometimes
imposes an awkward structure when doing so. It is also not always
possible in Wysteria to perform secure computations when the input
size can vary; e.g., it is difficult to write a sorting algorithm in
Wysteria where the (non-secret) size of the inputs can vary. Unrelated
to the expressiveness of the language, the design of PSL is such that
its formalization as \lang is far cleaner and simpler than Wysteria's
formalization. This is very useful and important, since the
formal semantics conveys the language's meaning, and is thus part of
the trusted computing base.

\paragraph{Other approaches}

\mwh{Nice syntax. Direct, not hacky. Cite/summarize}

\subsection{Outline}

The rest of this document describes \lang, a core calculus that
formalizes PSL and its behavior. In Section~\ref{sec:syntax} we
present the syntax of \lang, along discussion of how other features
available in PSL can be encoded in terms of the core
syntax. Section~\ref{sec:STsemantics} presents the
single-threaded semantics of \lang, while
Section~\ref{sec:MTsemantics} presents a sketch of the distributed
semantics. Section~\ref{sec:typing} presents the \lang type system and
states our core theoretical results, which is that for type-correct
programs, the single-threaded semantics simulates the distributed
semantics.

\begin{figure}[h]
  \centering
  \[\begin{array}{rlcll}
      \text{Principal} & A, B, C, ... \\
      \text{Principal set} & P, Q, ... \\
    \text{Execution modes} & m  & ::=  & P & \text{par mode with principal set $P$ (could be $\emptyset$)} \\
      \text{Protocol} & \psi & ::= & \cdot & \text{cleartext} \\
                       && \mid & \phi & \text{encryption format} \\
      \text{Types} & \tau & ::=  & \tnat^m_\psi & \text{a base type} \\
                       && \mid & \tpair{\tau}{\tau} & \text{pairs} \\
                       && \mid & \tsum{\tau}{\tau}{m} & \text{sums} \\
                       && \mid & \trec{\alpha}{\tau} & \text{(iso)recursive types} \\
                       && \mid & \alpha & \text{type variables (for recursive types)} \\
                       && \mid & \tfun{\tau}{\tau}{m} & \text{functions} \\
      \text{Protocol} & \psi & ::= & ... & \text{MPC type} \\
      \text{Expressions} & e & ::= & n & \text{naturals} \\
                       && \mid & x & \text{variables} \\
                       && \mid & \eread & \text{read from terminal}\\
                       && \mid & \ewrite{e} & \text{write to terminal}\\
                       && \mid & \epair{e_1}{e_2} & \text{pair construction}\\
                       && \mid & \eproj{i}{e} & \text{pair elem, }i \in \{1,2\}\\
                       && \mid & \eshare{\psi}{P}{Q}{e} & \text{$\psi$ shares of {\tnat} at $P$ to $Q$} \\
                       && \mid & \ereveal{\psi}{Q}{e} & \text{reveal result to $Q$ in format $\psi$}\\
                       && \mid & \ebinop{e_1}{e_2}  & \text{binary operation} \\
                       && \mid & \emux{e}{e_1}{e_2}  & \text{MPC conditional} \\
                       && \mid & \einj{i}{e} & \text{sum elem, }i \in \{1,2\}\\
                       && \mid & \econd{e}{e_1}{e_2}  & \text{sum elimination} \\
                       && \mid & \efold{\trec{\alpha}{\tau}}{e} & \text{rectype intro}\\
                       && \mid & \eunfold{e} & \text{rectype elim}\\
                       && \mid & \epar{P}{e} & \text{parallel evaluation}\\
                       && \mid & \elam{x}{e}  & \text{abstraction} \\
                       && \mid & \efix{f}{x}{e} & \text{recursive abstraction} \\
                       && \mid & \eapp{e_1}{e_2}  & \text{application} \\
                       && \mid & \elet{x}{e_1}{e_2}  & \text{sequencing} \\
  \end{array}
  \]
  \caption{Syntax of \lang}
  \label{fig:syntax}
\end{figure}

\newpage

\section{Syntax}
\label{sec:syntax}

The syntax of \lang is in Figure~\ref{fig:syntax}. Some derived
constructs (syntactic sugar) are given in Section~\ref{sec:derived}.

\subsection{Types}

There are two key concepts in types. First is their \emph{location},
designated $m$. Natural numbers, sums, and functions are located at
particular places; only here can they be computed on. Second (and in
addition), natural numbers can be \emph{encrypted} as secret
shares (with a different share at each party among the locations
$m$). We annotate the sharing protocol (GMW, Yao, etc.) as $\psi$; the
``cleartext'' protocol is $\cdot$ (or simply elided, to reduce
clutter).

Pairs and recursive types are not located in the same sense as the
other values. That is, all parties present in a computation when the
pair or recursive type is created can see its structure. They may not, however,
be able to access its values, as these could be located at particular
locations.

\subsection{Expressions}

Many of \lang's expression forms are standard, including
variables $x$; integers $n$; pair introduction $\epair{e_1}{e_2}$ and
elimination (projection) $\eproj{i}{e}$; sum introduction
$\einj{i}{e}$ and elimination $\econd{e}{e_1}{e_2}$ (where $x$ binds
in $e_1$ and $e_2$, only one of which is evaluated); recursive type
introduction $\efold{\trec{\alpha}{\tau}}{e}$ and elimination
$\eunfold{e}$; and abstraction $\elam{x}{e}$ (and recursive
abstraction $\efix{f}{x}{e}$), application $\eapp{e_1}{e_2}$, and
local variable binding $\elet{x}{e_1}{e_2}$. Some of these expressions
are located at $m$, per the discussion on types, above.

The expression $\epar{P}{e}$ says that $e$ may be computed at
principals $P$ in parallel (hence the syntax $\kw{par}$). That is,
every principal $A \in P$ may evaluate $e$. We say ``may'' here
because nesting such an expression in another $\kw{par}$ could shrink
the set of principals; e.g., $e$ will be evaluated only by those in
$P \cap Q$ in $\epar{Q}{\epar{P}{e}}$. We take this approach to avoid
otherwise odd behaviors that could arise.

We call the set of principals $P$ computing an expression in parallel
the \emph{mode} $m$. The semantics of many constructs depends on the
mode. A number $n$ created in mode $m$ (having type $\tnat^m$) can
only be computed on by principals $A \in m$; other principals do not
have access to it. This means that adding two numbers located at $m$
can only be done if principals $P \subseteq m$ are present; additional
principals wouldn't know what to do. The same goes for functions and
sums. The $\eread$ and $\ewrite{e}$ expressions read from and write to
the console. To be clear about \emph{which} console, the mode should
be a singleton principal.

On the other hand, it's possible that a variable $x$ will be bound at
$A$ but have a value only accessible to $B$ in it. Principal $A$ can
still manipulate a placeholder for $x$ but should never look at its
contents (i.e., to operate on it).

We conclude with the MPC-specific constructs in \lang.

Expression $\eshare{\psi}{P}{Q}{e}$ evaluates $e$ to a natural number,
and then $P$ secret-shares this number with the principals in $Q$. All
principals $P \cup Q$ must be present (i.e.,
$m \supseteq Q \cup P$). Moreover, $P$ must be a singleton, so we
are clear who is doing the sharing. The resulting value has type
$\tnat^Q_\psi$.

At this point, principals $Q$ can all mutually compute on the share,
using $\ebinop{e_1}{e_2}$ and $\emux{e}{e_1}{e_2}$. The former is
meant to be a generic binary operator, e.g., addition. The latter is a
multiplexor: We evaluate both $e_1$ and $e_2$ and select the result
based on whether $e$ is zero or non-zero. All sub-expressions must be
shared by the same principals, using the same protocol. Moreover, all
principals that have a share must be present when computing on
it. E.g., for numbers of type $\tnat^{\{A,B\}}_\psi$ to be added, both
$A$ and $B$ must be present. Even stronger, it must be \emph{exactly}
these principals which are present; we do not allow more, since other
principals would not be able to carry out the operation (they don't
have access to the share).

Finally, an MPC is completed by invoking $\ereveal{\psi}{Q}{e}$. This
takes a number share (among some set of principals $P$) and converts
it to type $\psi$, sharing the result among principals $Q$. Doing so
requires that all of $P \cup Q$ are present in the current mode so
that the shareholders can agree to send the value and the
result-receivers are there to receive it. In the degenerate case, the
protocol for $\psi$ is $\cdot$, which means decrypting the result to
cleartext. 

\subsection{Derived Expressions}
\label{sec:derived}

Some useful constructs present in PSL are not represented natively in
\lang, but they can be encoded using \lang features (or via
obvious language extensions). We present these features here, as well
as some examples.

\subsubsection{Message passing and share conversion}

Having a direct method of sending a (cleartext) message $m$ from $A$
to a set of principals $B$ and $C$ is written in PSL as \texttt{send\{A
    -> B,C\}} $m$. This primitive is easily encoded in \lang as
  \verb+reveal{:B,C}+ \verb+(share{:A -> A} +$m$\verb+)+.
Here, the value $m$ is shared to itself as cleartext, and then
distributed (again, as cleartext) to remote parties.

\lang's $\kw{reveal}$ construct supports revealing results to
cleartext, and converting between encryption/share types. For example,
$\ereveal{}{A}{e}$ computes the result of $e$ and reveals the final
result to $A$ as cleartext; $\ereveal{\kw{bgw}}{A}{e}$ instead
computes the result of $e$ and then converts it to $\kw{bgw}$ format
before sharing with $A$. In PSL \mwh{What is the conversion routine?}

\subsubsection{Generalized Mux and Reveal}
\label{sec:generalmux}

In \lang, a mux is restricted to returning encrypted
$\tnat$s, but in PSL more types are allowed. This generalization is
easily seen as a type-based translation. For example, here is how we
would encode a mux that returns a pair of encrypted 
$\tnat$s:
\begin{verbatim}
muxtopair e e1 e2 =
  let b = e in
  let (l1,r1) = e1 in
  let (l2,r2) = e2 in
  (b ? l1 : l2, b ? r1 : r2)
\end{verbatim}
Following this basic ``unrolling,'' we can likewise support
multiplexing on encrypted sums and recursive types.

We can similarly generalize the $\kw{reveal}$ construct to work on types
that are not numbers, but pairs, sums, or recursive types---just as we
mux on the elements of a pair here, we can \kw{reveal} elements
of a pair, too, with a single construct.

\subsubsection{Example: Lists of shares}

\newcommand{\twoprins}{\ensuremath{\{A,B\}}}

A common programming pattern is to compute over a list of encrypted
values (i.e., shares). 
%
Lists are native to PSL, and they can be encoded using sums, pairs,
and recursive types in \lang.

A list of shares between $A$ and $B$ would have type
$\trec{\alpha}{\tsum{(\tpair{\tnat^{\twoprins}_\psi}{\alpha})}{\tnat^{\twoprins}}{\twoprins}}$. In
short, a list of shares is either a pair, where the first element is a
\twoprins-share and the second is a list, or it's a normal integer
(i.e., the NULL terminator). Since neither the recursive type
nor the sum itself are encrypted, the list's size
(and the fact that's a list) is known and evident to both $A$ and $B$.

Here's an example of how $A$ and $B$ could create a list with a share
originating from each of them:
\begin{verbatim}
let l = par{A,B}
  let x = par{A} read in
  let y = par{B} read in
  let xs = share{bgw:A -> A,B} x in
  let ys = share{bgw:B -> A,B} y in
  fold (inj_1 (xs, fold (inj_1 (ys, fold (inj_2 0)))))
\end{verbatim}
Here's a function that sums the elements in the list:
\begin{verbatim}
let sum = par{A,B}
  fix f -> fun y ->
    case (unfold y) x.
      let (x1,x2) = x in
      x1 + (f x2)
    | share{bgw:A -> A,B} 0
\end{verbatim}
Notice that we have to explicitly share constants. In this case, the
\verb+0+ is shared by $A$ to $A$ and $B$.

Finally, we could apply \verb+sum+ to \verb+l+ and reveal the result
to both $A$ and $B$:
\begin{verbatim}
par{A,B}
  reveal{:A,B} (sum l)
\end{verbatim}

Suppose we tried to run \verb+sum+ just in \verb+par{A}+---what would
happen? It will fail because we require all parties having a share to
be present when the share is computed on (here, by
\verb!+!). Conversely, if we tried to run in mode \verb+par{A,B,C}+ it
would also fail: parties \emph{without} a share cannot carry out
computations on it.

\subsubsection{Wire Bundles and Solo Mode}
\label{sec:wirebundles}

A \emph{wire bundle} is a single logical object, but each party has
its own distinct value. In PSL, a wire bundle with $A$ and $B$ each
having their own integer value would be written \texttt{int\{isec:A,B\}}.

We represent a wire bundle in \lang as a product where each
element of the product is only visible to a single party. For
simplicity, consider the smallest one:
$\tpair{\tnat^{\{A\}}}{\tnat^{\{B\}}}$. To operate on a wire bundle we can use the
following sugared syntax:

\bigskip
\noindent
\verb+(all {A,B} +$x.e$\verb+) +$\epair{v_1}{v_2}$\\

Here, $\epair{v_1}{v_2}$ is a wire bundle with $v_1$ owned by $A$ and $v_2$ owned by
$B$. This syntax desugars to

\bigskip
\noindent
$
\epair{e[x \mapsto v_1][\alpha \mapsto A]}{e[x \mapsto v_2][\alpha \mapsto B])}\\
$

This says that the \texttt{all} construct runs $e$ with $x$
substituted with $v_1$ for the left side of the pair, and runs $e$
with $x$ substituted with $v_2$ on the right side. We also substitute
$\alpha$ with the respective principal $A$ or $B$.

We could use this construct to simplify the version of millionaire's
presented in Section~\ref{sec:overview}.

\bigskip
\noindent
\verb+1   let x = (all {A,B} x.par +$\alpha$\verb+ read) (0,0) in+\\
\verb+2   let s = (all {A,B} x.par +$\alpha$\verb!+{C,D} (share{!$\alpha$\verb+ -> C,D} x)) in+\\
\verb+3   let z = par{C,D,E} reveal{:E} par{C,D} (#1 s) < (#2 s) in+\\
\verb+4   par{E} write z+

\bigskip
Here, line 1 reads both $A$ and $B$'s inputs, storing them in the wire
bundle \texttt{A}. Notice that the code doing the reading is set as
par mode for $\alpha$, which will be separately run as $A$ and
$B$. Line 2 performs secret sharing from $A$ and/or $B$ to $C$ and
$D$. Lines 3 and 4 match the original program, comparing the two
results at $C$ and $D$, which send the revealed result to $E$, which
prints it.

We could simplify line 1 by specializing to the case that you always
run in par mode for a single principal, i.e., \emph{solo mode} such
that 
\verb+(solo {+$A,B$\verb+} +$x.e$\verb+) +$\epair{v_1}{v_2}$
desugars to
$
\epair{\epar{\{A\}}{e}[x \mapsto v_1]}{\epar{\{B\}}{e[x \mapsto v_2]}}
$.
This is a little cleaner, so the first line of millionaires would become
\begin{verbatim}
let x = (solo {A,B} x.read) (0,0) in
\end{verbatim}
You can imagine generalizing \texttt{all} and \texttt{solo} to wire
bundles of arbitrary (but fixed) size. Another useful pattern is to convert a single wire bundle to a list,
and then supporting equivalent constructs like \texttt{wfold}.

An interesting twist is to encode a wire bundle of lists as a list of
wire bundles. Doing so ensures that the list is the same length at
both parties. For example, a list of wire
bundles private to $A$ and $B$ would have type 
$\trec{\alpha}{\tsum{(\tpair{(\tpair{\tnat^{\{A\}}}{\tnat^{\{B\}}})}{\alpha})}{\tnat^{\twoprins}}{\twoprins}}$.
Summing each principal's components would look like this:
\begin{verbatim}
let sum2 = par{A,B}
  fix f.lam y.
    case (unfold y) x.
      let ((xa,xb),x2) = x in
      let (sa,sb) = f x2 in
      let l = par{A} sa+xa in
      let r = par{B} sb+xb in
      (l,r)
    | (par{A} 0,par{B} 0)
\end{verbatim}
We could generalize this pattern to work for generic functions (rather
than being specialized to summation, as here).

% For example, they could run in full par mode
% (with multiple principals) but then accesses to the individual elements
% could be dropped to a single mode, for those accesses. For this, you
% need a map from principal $\alpha$ to index in the pair for $\alpha$'s
% value. For the accessor in the code, you use the map to extra from the
% pair. Maybe use ``self'' and not $\alpha$.

\subsubsection{Encrypted Sums}

PSL does not currently support sum types directly; it just supports
lists. However, as shown in \lang, they are not hard to add. Indeed,
we can also support a kind of \emph{encrypted sum} without much extra
work. 

An encrypted sum can be represented as a value of the type
$\tpair{\tnat^m_\psi}{(\tpair{\tau_1}{\tau_2})}$. Here, the first
element is a boolean that represents whether the ``real'' portion of
the sum is the left side. We can introduce (encrypt) an existing sum
by doing the following:
\begin{verbatim}
let sharesum e def_lhs def_rhs =
  case e 
    x.(share 1,x,def_rhs)
  | x.(share 0,def_lhs,x)
\end{verbatim}
We can eliminate an encrypted sum by using a mux, which forces us
to produce a $\tnat^m_\psi$, to ensure obliviousness:
\begin{verbatim}
let muxshare e t_fun f_fun =
  let (x,(l,r)) = e in
  x ? t_fun l : f_fun r
\end{verbatim}
While \verb+t_fun+ and \verb+f_fun+ are notated as functions, instead
it makes sense to make these proper syntax in \lang, with a
binding form like $\kw{match}$.

Another question: What if we ``encrypted'' the sum type that defines
the list (per the encoding given earlier)? This would imply that both the left
and right side of the sum are always possible, so both would be
maintained. However, since this is a recursive type and we cannot
support infinitely sized data structures, we could not actually
construct elements of this type! The reason you can construct
recursive data structures at all is that there is eventually a base
case, i.e., just one side of the sum, here the $\tnat^{\{A,B\}}_\psi$
part. But when you are required provide \emph{both} sides, you never
get to the base case (only). You could do a constant-length list, but
not using a recursive type; you'd have to do a finite (by hand)
unrolling.

\subsubsection{Other Differences with PSL}

There are some other differences between \lang and PSL that are not
particularly consequential; we touch on them here.

PSL has effect annotations on function arrows indicating when
\texttt{reveal} is called. The reason is to alert someone reading an
API, which will include a function's type but not show its code, that secret values could
potentially be revealed by calling the function. This is a kind of
checked documentation. Such annotations are straightforward to
implement and are not consequential in the metatheory we are
interested in, so we do not include them in \lang.

PSL has global principal declarations, e.g., of the form
\texttt{principal A}. It also permits defining sets of principals,
e.g., as \texttt{principal A[2]} for a set of 2 principals. \lang does
not require these; rather, it assumes an existing pool of principal
names. PSL programs can be viewed as being defined in a global
$\kw{par}$ block involving the declared principals; \lang programs
require making the $\kw{par}$ block explicit.

\mwh{Anything else ...}

\subsection{Future work: First-class Principals and Dependent and
  Polymorphic Types}
\label{sec:deptypes}

One aspect of PSL not encoded here is the idea of
\emph{principals as data}, and more generally, having a first-class
representation of a set of principals. Run-time principal sets are
useful for writing computations that are generic over the identity and
number of principals involved in a computation. For example, you could
make a generic function that computes the maximum of any number of
parties' input values.

For principal sets to work, we must generalize constructs in the
language that are currently operating over sets of principals $P$. At
present, the typing and compatibility judgments rely on knowing the
precise makeup of a principal set. This would have to change to permit
unknown principal sets whose contents are governed by set
constraints. A simple manifestation of this point is that
$\ereveal{\psi}{Q}{e}$ should be $\ereveal{\psi}{e_Q}{e}$
where $e_Q$ is a principal set; likewise for $\kw{share}$
and $\kw{par}$. We would also need dependent types; for example if we
had a run-time principal set stored in variable $s$, then we need to
express types like $\tnat^s$, which indicates a type compatible with
all principals in $s$. One interesting point is that we would like
set-indexed recursive datastrutures like lists, e.g., so that each
element of a list could belong to a different principal. This
generalized the fixed-size notion of wire bundles given above.

\mwh{Refer to Wysteria as having these, and that we can use its
  metatheory for inspiration}
\mwh{TODO: More about polymorphism. Describe \texttt{this}.}

We leave it to future work to flesh out the details. 

\section{Semantics (Single Threaded)}
\label{sec:STsemantics}

\begin{figure}
  \[\begin{array}{rlcll}
      \text{Store} & \sigma & \in & \mathbf{Var} \rightharpoonup \mathbf{Val}\\
      \text{Locatable value} & u & ::=  & n & \text{numbers} \\
                             && \mid & \einj{i}{v} & \text{sum elem, }i \in \{1,2\}\\
                             && \mid & \vclos{\env}{e}  & \text{closure} \\
                             && \mid & \vshare{Q}{\psi}{e} & \text{$\psi$-$Q$ share of evaluating $e$} \\
      \text{Value} & v  \in \mathbf{Val} & ::=  & \epair{v_1}{v_2} & \text{pair value}\\
                       && \mid & \efold{\trec{\alpha}{\tau}}{v} & \text{rectype value}\\
                   && \mid & \vloc{u}{P} & \text{located value}\\
                   && \mid & \vcrash & \text{bogus ``crash" value}\\
    \end{array}
  \]

\[\begin{array}{l@{~~=~~}l}
    \locof{P}{n} & \vloc{n}{P} \\
    \locof{P}{\einj{i}{v}} & \vloc{(\locof{P}{v})}{P} \\
    \locof{P}{\vclos{\env}{\elam{x}{e}}} & \vloc{\vclos{\env}{\elam{x}{e}}}{P}\\
    \locof{P}{\vshare{Q}{\psi}{e}} & \vloc{\vshare{Q}{\psi}{e}}{P}
    \\ \\
    \locof{P}{\epair{v_1}{v_2}} & \epair{\locof{P}{v_1}}{\locof{P}{v_2}}\\
    \locof{P}{ \efold{\trec{\alpha}{\tau}}{v}} &  \efold{\trec{\alpha}{\tau}}{\locof{P}{v}}\\
    \locof{P}{\vloc{u}{Q}} & \locof{R}{u} \qquad\text{where }R = P \cap Q\\
    \locof{P}{\vcrash} & \vcrash
    \\ \\
    % \locof{P}{\sigma} & \{ x \mapsto v' \mid \sigma(x) = v \land \locof{P}{v} = v' \}\\
    % \multicolumn{2}{c}{}\\
    \getat{P}{(\vloc{u}{Q})} & u \qquad \text{where }Q \vdash P\\
    \getat{P}{v} & v \qquad \text{for all other $v$ syntactic forms}\\
  \end{array}
\]
\caption{Semantics auxiliaries}
\label{fig:auxsem}
\end{figure}

\begin{figure}
$$
\begin{array}{c}
    \inferrule*[lab=E-Nat]
    {
    }
    {
    \eval{\env}{m}{n}{\vloc{n}{m}}
    }
    \qquad

    \inferrule*[lab=E-Var]
    {
    \env(x) = v_0\\\\ \locof{m}{v_0} = v
    }
    {
    \eval{\env}{m}{x}{v}
  }\qquad

  \inferrule*[lab=E-Read]
    {
  m\text{ is a singleton}
    }
    {
    \eval{\env}{m}{\eread}{\vloc{n}{m}}
    }
    \qquad

      \inferrule*[lab=E-Write]
      {
      m\text{ is a singleton}\\\\
      \eval{\env}{m}{e}{v}\\
  \getat{m}{v} = n
    }
    {
    \eval{\env}{m}{\ewrite{e}}{\vloc{n}{m}}
    }
    \\ \\

      \inferrule*[lab=E-ParEmp]
    {
  m \cap P = \emptyset
    }
    {
    \eval{\env}{m}{\epar{P}{e}}{\vcrash}
    } \qquad
  
      \inferrule*[lab=E-ParStep]
    {
  Q = m \cap P\\
  Q \not= \emptyset \\\\
    \eval{\env}{{Q}}{e}{v}
    }
    {
    \eval{\env}{m}{\epar{P}{e}}{v}
    } \qquad

  \inferrule*[lab=E-Pair]
    {
    \eval{\env}{m}{e_1}{v_1}\\\\
    \eval{\env}{m}{e_2}{v_2}
    }
    {
    \eval{\env}{m}{\epair{e_1}{e_2}}{\epair{v_1}{v_2}}
    }\\ \\
    
    \inferrule*[lab=E-Access]
    {
    \eval{\env}{m}{e}{\epair{v_1}{v_2}}
    }
    {
    \eval{\env}{m}{\eproj{i}{e}}{v_i}
    }\qquad

    \inferrule*[lab=E-inj]
    {
    \eval{\env}{m}{e}{v}
    }
    {
    \eval{\env}{m}{\einj{i}{e}}{\vloc{(\einj{i}{v})}{m}}
    } \qquad
    
    \inferrule*[lab=E-Match]
    {
    \eval{\env}{m}{e}{v_0}\\
    \getat{m}{v_0} = \einj{i}{v_i}\\\\
    \eval{\env[x\mapsto v_i]}{m}{e_i}{v}
    }
    {
      \eval{\env}{m}{\econd{e}{e_1}{e_2}}{v}
    }\\ \\

    \inferrule*[lab=E-Fold]
    {
    \eval{\env}{m}{e}{v}
    }
    {
    \eval{\env}{m}{\efold{\trec{\alpha}{\tau}}{e}}{\efold{\trec{\alpha}{\tau}}{v}}
    } \qquad

    \inferrule*[lab=E-Unfold]
    {
    \eval{\env}{m}{e}{\efold{\trec{\alpha}{\tau}}{v}}
    }
    {
    \eval{\env}{m}{\eunfold{e}}{v}
    } \qquad

    \inferrule*[lab=E-Share]
    {
    m \vdash P\\ P\text{ is a singleton}  \\\\
    m \vdash Q\\ P \cup Q \vdash m\\\\
    \eval{\env}{m}{e}{v}\\
    \getat{P}{v} = n  
    }
    {
  \eval{\env}{m}{\eshare{\psi}{P}{Q}{e}}{\vloc{\vshare{Q}{\psi}{n}}{Q}}
    }\\ \\
    
    \inferrule*[lab=E-BinopNat]
    {
  \eval{\env}{m}{e_1}{v_1}\\\\
    \eval{\env}{m}{e_2}{v_2}\\\\
  \getat{m}{v_1} = n_1\\\\
  \getat{m}{v_2} = n_2\\\\
  n = n_1 \oplus n_2
    }
    {
    \eval{\env}{m}{\ebinop{e_1}{e_2}}{\vloc{n}{m}}
    }\qquad

      
    \inferrule*[lab=E-BinopShare]
    {
  \eval{\env}{m}{e_1}{v_1}\\\\
    \eval{\env}{m}{e_2}{v_2}\\\\
  \getat{m}{v_1} = \vshare{m}{\psi}{e'_1} \\\\
  \getat{m}{v_2} = \vshare{m}{\psi}{e'_2} \\\\
  v = \vloc{\vshare{m}{\psi}{\ebinop{e'_1}{e'_2}}}{m}
    }
    {
    \eval{\env}{m}{\ebinop{e_1}{e_2}}{v}
    }\qquad

    \inferrule*[lab=E-Mux]
    {
  \eval{\env}{m}{e}{v_0}\\
  \getat{m}{v_0} = \vshare{m}{\psi}{e'} \\\\
  \eval{\env}{m}{e_1}{v_1}\\
  \getat{m}{v_1} = \vshare{m}{\psi}{e'_1} \\\\
    \eval{\env}{m}{e_2}{v_2}\\
  \getat{m}{v_2} = \vshare{m}{\psi}{e'_2} \\\\
  v = \vloc{\vshare{m}{\psi}{\emux{e'}{e_1'}{e_2'}}}{m}
    }
    {
    \eval{\env}{m}{\emux{e}{e_1}{e_2}}{v}
    }\\ \\

    \inferrule*[lab=E-RevealPlain]
  {
  m \vdash P \cup Q\\
  P \cup Q \vdash m\\\\
    \eval{\env}{m}{e}{v_0}\\
    \getat{P}{v_0} = \vshare{P}{\psi}{e_0}\\\\
    \eval{\env}{m}{e_0}{v}\\
    n = \getat{P}{v} 
    } 
    {
    \eval{\env}{m}{\ereveal{}{Q}{e}}{\vloc{n}{Q}}
  }\qquad

      \inferrule*[lab=E-Conv]
  {
    m \vdash P \cup Q\\ P \cup Q \vdash m\\\\
    \eval{\env}{m}{e}{v_0}\\
    \getat{P}{v_0} = \vshare{P}{\psi}{e_0}
    } 
    {
    \eval{\env}{m}{\ereveal{\phi}{Q}{e}}{\vloc{\vshare{Q}{\phi}{e_0}}{Q}}
    }\\\\
    
   \inferrule*[lab=E-Abs]
    {
    }
    {
    \eval{\env}{m}{\elam{x}{e}}{\vloc{\vclos{\env}{\elam{x}{e}}}{m}}
    }\qquad
   
    \inferrule*[lab=E-Fix]
    {
    }
    {
    \eval{\env}{m}{\efix{f}{x}{e}}{\vloc{\vclos{\env}{\efix{f}{x}{e}}}{m}}
    }\\\\

    \inferrule*[lab=E-App]
    {
  \eval{\env}{m}{e}{v'}\\
    \getat{m}{v'} = \vclos{\env'}{\elam{x}{e'}}\\\\
  \eval{\env}{m}{e_1}{v_1}\\
  \eval{\env'[x \mapsto v_1]}{m}{e'}{v}\\
    }
    {
    \eval{\env}{m}{\eapp{e}{e_1}}{v}
    }\qquad

  
    \inferrule*[lab=E-FixApp]
    {
  \eval{\env}{m}{e}{v'}\\
    \getat{m}{v'} = \vclos{\env'}{\efix{f}{x}{e'}}\\\\
 \eval{\env}{m}{e_1}{v_1}\\
 \eval{\env'[f \mapsto v'][x \mapsto v_1]}{m}{e'}{v}\\
    }
    {
    \eval{\env}{m}{\eapp{e}{e_1}}{v}
    }\\ \\

    \inferrule*[lab=E-Let]
    {
    \eval{\env}{m}{e_1}{v_1}\\\\
    \eval{\env[x\mapsto v_1]}{m}{e_2}{v_2}
    }
    {
    \eval{\env}{m}{\elet{x}{e_1}{e_2}}{v_2}
  } \qquad


  \inferrule*[lab=E-CrashOK]
    {
    \eval{\env}{m}{e}{\vcrash}
    }
    {
    \eval{\env}{m}{\eproj{i}{e}}{\vcrash}\\\\
    \eval{\env}{m}{\eunfold{e}}{\vcrash}
    }\\\\

\end{array}
    $$
\caption{Operational Semantics (Single threaded, big step)}
\label{fig:sem}
\end{figure}

We define a big-step operational semantics. The judgment
$\eval{\env}{m}{e}{v}$ states that program
$e$ evaluates under mode $m$ and store $\env$ to a value
$v$. The rules are given in Figure~\ref{fig:sem}.  This is the
``single threaded semantics'' in that we don't have independently
executing parties; rather, we simulate the group of principals
$m$ executing in lockstep.

\subsection{Located values}

To be clear about which values reside on which parties' hosts, we have
a special value form to indicates this. Values are defined in
Figure~\ref{fig:auxsem}. The form $\vloc{u}{P}$ indicates that $u$ is
only visible to principals $A \in P$. Since not all values need to be
explicitly located (if they are in scope, they implicitly are
accessible), we distinguish \emph{locatable} values $u$.  Values for
numbers, sums, and closures are located, and otherwise standard
(closures have an explicit environment---we don't use
substitution). Values for pairs and recursive types are not located,
but are also standard. The value $\vshare{Q}{\psi}{e}$ represents a
circuit for a secure computation under scheme $\psi$ to be performed
by principals in $B \in Q$. Notice that it contains an expression $e$,
not a value $v$; this represents a ``suspended'' computation.

We also have a special value $\vcrash$ that stand in for values
present at other hosts. This is particularly important, when we get to
the distributed semantics, for container values (pairs and recursive
types). Such values are not ``located'' so all hosts must be able to
``access'' them. Non-containers, however, cannot be eliminated if they
are not present at the host. The $\vcrash$ value does not appear in
source programs and arises when computing a $\epar{Q}{e}$ such that
$Q$ is empty; we see this in the next subsection.

In the same figure are two functions over values: $\locof{P}{...}$ and
$\getat{P}{v}$. The first is a transformation that relocates a value
(or its locatable components) to $P$. For locatable, but not located, values $u$,
we annotate them with $P$. For compound forms we recurse inside them
(sums, pairs). We do not need to locate closure environments; variable
lookup from these environments (and control of the location of the
closure itself) does the job.  For located values $\vloc{u}{Q}$ we
relocate $u$'s contents and update its location to be $P$ intersected
with $Q$. Note that this intersection could be the $\emptyset$ which
indicates an inaccessible value.

The function $\getat{v}{P}$ attempts to strip off the outermost
location designator so the value $v$ can be computed on. For all
values other than $\vloc{u}{Q}$ this is a no-op. For these, we confirm
that the location $Q$ is compatible with the accessing environment,
written $P \vdash Q$, and given in Figure~\ref{fig:aux} (there it says
$m \vdash m'$). Extrapolating: $Q \vdash P$ is never true if the value's
location $Q$ is $\emptyset$; it will be true if $Q$ contains all
principals in the requested mode $P$.

\subsection{Operational rules}

Now we turn to the rules. An invariant of the semantics is that
\emph{If $\eval{\env}{m}{e}{v}$ then $v$ is compatible with $m$}. By
"compatible with $m$," we mean $v$ is located at principal set(s)
$P \subseteq m$. This means $v$ could be located at $\emptyset$,
making it inaccessible to code subsequently trying to use it. Note
that located values $u$ need to be explicitly annotated. For example,
\rulelab{E-Nat} could not just evaluate to $n$ because the lack of an
explicit location basically means \emph{every} location, which is not
compatible with a specific $m$. Therefore, it evaluates to
$\vloc{n}{m}$ instead. Rules \rulelab{E-Abs}, and \rulelab{E-Fix} are
similarly standard except that they explicitly locate their
result. \rulelab{E-Var} retrieves a variable's value from the
environment but narrows its location to be compatible with the current
mode $m$. (Indeed, we don't need to explicitly locate the closure's
environment $\env$ in \rulelab{E-Abs} and \rulelab{E-Fix} because if
we ever use variables from the $\env$ later, their accessibility will
be trimmed to the current mode by \rulelab{E-Var}.) \rulelab{E-Read}
and \rulelab{E-Write} only work in a singleton context---we need to
know which principal is reading from/writing to their terminal, and we
annotate the result with that principal.

Rules \rulelab{E-Pair}, \rulelab{E-Access}, \rulelab{E-Fold},
\rulelab{E-Unfold}, and \rulelab{E-Let} are completely standard. They
do not operate on located values, so there is nothing special to do.
Rules \rulelab{E-Inj}, \rulelab{E-Match}, \rulelab{E-BinopNat},
\rulelab{E-App}, and \rulelab{E-FixApp} differ from their standard
counterparts in that they refer to $\getat{m}{v}$ in the premise---as
elimination forms, they have to have to strip off any location
information to use the value. Doing so will fail if the value is
not available to (located at) \emph{all} principals in the required
mode $m$; the type system aims to rule out this sort of problem. These
rules locate the final result at $m$.

Now consider \rulelab{E-ParEmp} and \rulelab{E-ParStep}, which
evaluate $\epar{P}{e}$. A $\kw{par}$ evaluates $e$ in mode
$Q = m \cap P$ to produce $v$; i.e., only those principals in $P$
\emph{also} present in $m$ will run $e$. Per \rulelab{E-ParEmp}, if
$Q$ is empty, then no evaluation takes place, and $\vcrash$ is
returned. (The alternative of running $e$ with $\emptyset$ as the
mode is not equivalent; see discussion below.)  Otherwise, evaluation
takes place in mode $Q$ and its result $v$ is returned (which will be
compatible with $m$ because it's compatible with $Q$, a subset).

Let's consider the remaining rules, which focus on multiparty
computation. \rulelab{E-Share} models principal $A \in P$
``encrypting'' an integer, sharing it with principals $Q$. Expression
$e$ is evaluated in the current mode $m$, but the direction is that
just $P$ is doing the sharing; hence we check that $P$ is present in
$m$, and $v$ is located
on $P$. We then encapsulate the extracted number $n$ in a share value,
split between principals at $Q$, which must be present in $m$
(ensuring our invariant on the location of final values). We locate
this result at $Q$. No principals other than $P \cup Q$ may be
present. 

\rulelab{E-BinopShare} permits multiparty computation on shares. It
makes sure both arguments are available to exactly the executing hosts
and the encapsulates the ``suspended'' computation in a share
itself. \rulelab{E-Mux} produces a multiplexor on shares. It evaluates
its arguments $e$, $e_1$, and $e_2$ to shares, and then constructs a
circuit involving all three.

\rulelab{E-RevealPlain} eliminates shares by ``forcing'' the suspended
computation in the given share, revealing it to cleartext. We require
that the principals $P$ who each have a share are present in $m$, and
likewise the principals in $Q$ to whom the final result is sent (and
no others). \rulelab{E-Conv} converts a share from type $\psi$ among
principals $P$ to be of type $\phi$ among principals $Q$ instead.

Lastly, \rulelab{E-CrashOK} is an elimination rule for $\vcrash$
values. This rule permits converting non-located containers---pairs
and recursive types---into $\vcrash$ values. However, there are no
elimination rules for $\vcrash$ values having located types, i.e.,
numbers, sums, or functions. A program that tries to use a crash value
as one of these will be stuck.

% \paragraph{Discussion}

% Rule~\rulelab{E-ParEmp} indicates that if the ``present'' set of
% principals $Q$ is $\emptyset$ then $e$ should be skipped, and an
% (inaccessible) value returned instead. The alternative is to ``run''
% $e$ despite having no principals present.

% One important justification for skipping $e$ is that this is what the
% distributed semantics will do, in a general sense (see the next
% section for more on this). In particular, if
% we have $\epar{\{A,B\}}{e}$ running in mode $m = \{ a, b, c \}$ then
% what should $C$ do while $A$ and $B$ are each running $e$? The
% assumption is that it will skip it. Another way to put this: We can
% think of a principal's ``slice'' of an expression as that expression
% with $\kw{par}$ sets intersected with that principal. So on $C$ it
% would be $\epar{\{A,B\} \cap \{C\}}{e}$ or in other words the
% principal set is empty---no relevant principals are ``here''---and we
% don't run it. This implies that a $\kw{par}$ block with an empty
% principal set should be skipped (and the $\vcrash$ value returned as a
% placeholder). 

% The alternative is weird. If no-principal blocks should be executed,
% how should that be done? The meaning is that no principal sets are
% ``here'' to run the block, so who exactly is doing the running? 

% Suppose we choose to run $e$ with an empty set, say on each principal
% $A$, $B$, and $C$. Doing so could have failures or non-termination. For the
% former, it could reach a $\kw{reveal}$, $\kw{share}$, $\kw{read}$,
% or $\kw{write}$, each of which require certain principals to be
% present. Since no principals are present, evaluation will get stuck
% at these constructs. This is odd, once again, from the perspective of
% the distributed semantics: We would not expect to run $A$'s blocks on
% $B$ with an empty principal set because doing so will fail.

% You might argue that somehow a $\kw{par}$ block should be run, even
% with no principals (somewhere) for its termination behavior. The
% analogy would be that you have $\epar{A}{...}$ followed by
%   $\epar{B}{...}$ and the first doesn't terminate in the ST semantics,
%   precluding running the second block there, whereas in the MT the
%   second will run. The argument is somehow that if you don't ``run''
%   $\epar{\emptyset}{...}$ blocks, you won't match this kind of
%     behavior, if the $...$ would fail to terminate. But I don't think this
% is either here nor there; the simulation theorem is about computations
% that terminate; we are not obligated to reason about non-terminating
% programs. And in any case it's about ``real'' computations; not ones
% that are contrived.

\subsection{Understanding Visibility}

Here are several examples illustrating how multi-party value
visibility is managed by the semantics.

\paragraph{Example 1}
\begin{verbatim}
par{A,B}
  let x = par{B} (0,0) in
  par{A} let y = #1 x in y
\end{verbatim}
Running this example will produce $\vcrash$, but since the value is never accessed by the program,
this is a fine result. The pair bound in \texttt{x} is only visible to
$B$, but is accessed in a block run only by $A$. When $x$ is looked up
in that latter block, $A$ will have no specific value for it but the
\texttt{\# 1} operation will be allowed to proceed, also producing
$\vcrash$, as pairs are not located.

\paragraph{Example 2}
\begin{verbatim}
(fun x -> par{C} x) 
  (par{A,B,C} reveal{A,B}{C} (share{A,B}{A} 0))
\end{verbatim}
This program has $A$ share $0$ with itself and $B$, and then reveal
the results of that share to $C$. These operationsn are 
This is allowed by the type system. The $\kw{reveal}$ directs $A$ and
$B$ to reveal their result to $C$. The result is returned from the
$\kw{par}$ block and successful accessed by $C$ in the called function.

\paragraph{Example 3}
\begin{verbatim}
par(A,B) 
  let x = par(A) 0 in
  let y = par(B) 1 in
  par(A) 
    let z = (x,y) in #1 z
\end{verbatim}
This program works because even though $y$ is not available at $A$
where the pair is constructed, it is never accessed.

\paragraph{Example 4}
\begin{verbatim}
par{A,B}
  let x0 = par{B} 0 in
  let x1 = (fun x -> par{A} (x,1)) x0 in
  x0
\end{verbatim}
Running this program produces the value $0$ located at $B$. The
variable \texttt{x1} will be bound to $\epair{\vcrash}{1}$, since the
first element is not accessible at $A$, but this is not the value that
is ultimately returned.

\paragraph{Example 5}
\begin{verbatim}
let x = par{A,C} 0 in
let f = par{A,B} fun y -> (x,y) in
par{B} f 1
\end{verbatim}
This will create a variable located at $A$ and $C$; then it creates a
closure at $A$ and $B$. This means that the call to \texttt{f} with
only $B$ present, on the last line, will return a pair $\epair{0}{1}$
where $0$ is located at $\emptyset$ (i.e., it is inaccessible to
everyone) and $1$ is located at $B$. Note that the last line
\texttt{par\{B\} f 1} would fail if changed to \texttt{par\{C\} f
  1}. This is because the closure $f$ is not located at $C$ so it
can't be called from there. If the last line was \texttt{par\{A\} f 1}
then $\epair{0}{1}$ would be returned again, but this time $0$ and $1$
would both be located at $A$.

\subsection{Small-step Semantics}

In preparation for the multi-threaded semantics, we need a small-step
version of the big-step semantics presented above. Judgment
$\seval{\env}{e}{m}{\env'}{e'}$ states that in mode $m$ and
environment $\env$, expression $e$ steps to expression $e'$ and
environment $\env'$. The rules are defined in
Figure~\ref{fig:small-sem}. 

We define \emph{evaluation contexts} $\ctxt$ at the bottom of the
figure. Evaluation contexts define left-to-right, 
call-by-value evaluation in the standard manner per the rule
\rulelab{S-Ctxt}. We do not include context $\epar{P}{\ctxt}$ in the
definition of contexts; i.e., \rulelab{S-Ctxt} does not cover
evaluation within par blocks. Instead, this is handled by
\rulelab{S-ParStep}, which is essentially a variation of
\rulelab{S-Ctxt} but it adjusts the mode when evaluating $e$ according
to $P$. Most of the rest of the rules are just the small-step versions
of the previous big-step rules. One interesting rule is
\rulelab{S-Reveal}---here we evaluate the contents of the share $e_0$
in one fell swoop, using the big-step semantics.

\begin{figure}
$$
\begin{array}{c}
 
    \inferrule*[lab=S-Nat]
    {
    }
    {
    \seval{\env}{m}{n}{\env}{\vloc{n}{m}}
  }\qquad

  \inferrule*[lab=S-Var]
    {
    \env(x) = v_0\\\\ \locof{m}{v_0} = v
    }
    {
    \seval{\env}{m}{x}{\env}{v}
  }\qquad

  \inferrule*[lab=S-Read]
    {
  m\text{ is a singleton}
    }
    {
    \seval{\env}{m}{\eread}{\env}{\vloc{n}{m}}
    }
    \qquad

      \inferrule*[lab=S-Write]
      {
      m\text{ is a singleton}\\\\
  \getat{m}{v} = n
    }
    {
    \seval{\env}{m}{\ewrite{v}}{\env}{\vloc{n}{m}}
    }
    \\ \\

  \inferrule*[lab=S-ParEmp]
  {
  m \cap P = \emptyset\\
  }
  {
  \seval{\env}{m}{\epar{P}{e}}{\env}{\vcrash}
  } \qquad
  
  \inferrule*[lab=S-ParStep]
  {
  m \cap P = Q\\
  Q \not= \emptyset\\
  \seval{\env}{Q}{e}{\env'}{e'}
  }
  {
  \seval{\env}{m}{\epar{P}{e}}{\env'}{\epar{P}{e'}}
  } \qquad


  \inferrule*[lab=S-ParVal]
  {
  m \cap P \not= \emptyset\\
  }
  {
  \seval{\env}{m}{\epar{P}{v}}{\env}{v}
  } \\ \\

      \inferrule*[lab=S-Let]
    {
    }
    {
    \seval{\env}{m}{\elet{x}{v_1}{e_2}}{\env[x\mapsto v_1]}{e_2}
  } \qquad

  \inferrule*[lab=S-Access]
    {
    }
    {
    \seval{\env}{m}{\eproj{i}{\epair{v_1}{v_2}}}{\env}{v_i}
    }\\ \\

  \inferrule*[lab=S-Inj]
  {
  }
  {
  \seval{\env}{m}{\einj{i}{v}}{\env}{\vloc{(\einj{i}{v})}{m}}
  }\qquad
  
    \inferrule*[lab=S-Match]
    {
    \getat{m}{v} = \einj{i}{v_i}
    }
    {
      \seval{\env}{m}{\econd{v}{e_1}{e_2}}{\env[x\mapsto v_i]}{e_i}
    }\\ \\

    \inferrule*[lab=S-Unfold]
    {
    }
    {
    \seval{\env}{m}{\eunfold{(\efold{\trec{\alpha}{\tau}}{v})}}{\env}{v}
    } \qquad

    \inferrule*[lab=S-Share]
    {
    m \vdash P \cup Q\\
    P\text{ is a singleton}  \\ P \cup Q \vdash m\\
    \getat{P}{v} = n  
    }
    {
  \seval{\env}{m}{\eshare{\psi}{P}{Q}{v}}{\env}{\vloc{\vshare{Q}{\psi}{n}}{Q}}
    }\\ \\
    
    \inferrule*[lab=S-BinopNat]
    {
  \getat{m}{v_1} = n_1\\
  \getat{m}{v_2} = n_2\\
  n = n_1 \oplus n_2\\
    }
    {
    \seval{\env}{m}{\ebinop{v_1}{v_2}}{\env}{\vloc{n}{m}}
    }\qquad
    
    \inferrule*[lab=S-BinopShare]
    {
  \getat{m}{v_1} = \vshare{m}{\psi}{e_1} \\
  \getat{m}{v_2} = \vshare{m}{\psi}{e_2} \\
    }
    {
    \seval{\env}{m}{\ebinop{v_1}{v_2}}{\env}{\vloc{\vshare{m}{\psi}{\ebinop{e_1}{e_2}}}{m}}
    }\\\\

    \inferrule*[lab=S-Mux]
    {
  \getat{m}{v_0} = \vshare{m}{\psi}{e'} \\\\
  \getat{m}{v_1} = \vshare{m}{\psi}{e'_1} \\\\
  \getat{m}{v_2} = \vshare{m}{\psi}{e'_2} \\\\
  v = \vloc{\vshare{m}{\psi}{\emux{e'}{e_1'}{e_2'}}}{m}
    }
    {
    \seval{\env}{m}{\emux{v_0}{v_1}{v_2}}{\env}{v}
    }\qquad

    \inferrule*[lab=S-RevealPlain]
  {
    m \vdash P \cup Q\\ P \cup Q \vdash m\\\\
    \getat{P}{v_0} = \vshare{P}{\psi}{e_0}\\\\
    \eval{\env}{m}{e_0}{v}\\\\
    n = \getat{P}{v} 
    } 
    {
    \seval{\env}{m}{\ereveal{}{Q}{v_0}}{\env}{\vloc{n}{Q}}
  }\qquad

    \inferrule*[lab=S-Conv]
  {
    m \vdash P \cup Q \\ P \cup Q \vdash m\\\\
  \getat{P}{v_0} = \vshare{P}{\psi}{e_0}\\\\
  v = \vloc{\vshare{Q}{\phi}{e_0}}{Q}
    } 
    {
    \seval{\env}{m}{\ereveal{\phi}{Q}{v_0}}{\env}{v}
  }\\ \\  

     \inferrule*[lab=S-Abs]
    {
    }
    {
    \seval{\env}{m}{\elam{x}{e}}{\env}{\vloc{\vclos{\env}{\elam{x}{e}}}{m}}
    }\qquad

    \inferrule*[lab=S-Fix]
    {
    }
    {
    \seval{\env}{m}{\efix{f}{x}{e}}{\env}{\vloc{\vclos{\env}{\efix{f}{x}{e}}}{m}}
    }\\\\

    \inferrule*[lab=S-App]
    {
    \getat{m}{v_0} = \vclos{\env'}{\elam{x}{e'}}
    }
    {
    \seval{\env}{m}{\eapp{v_0}{v_1}}{\env'[x \mapsto v_1]}{e'}
    }\qquad

  
    \inferrule*[lab=S-FixApp]
    {
    \getat{m}{v_0} = \vclos{\env'}{\efix{f}{x}{e'}}
    }
    {
    \seval{\env}{m}{\eapp{v_0}{v_1}}{\env'[f \mapsto v'][x \mapsto v_1]}{e'}
    }\\ \\

    \inferrule*[lab=S-CrashOK]
    {
    }
    {
    \seval{\env}{m}{\eproj{i}{\vcrash}}{\env}{\vcrash}\\\\
    \seval{\env}{m}{\eunfold{\vcrash}}{\env}{\vcrash}
    }\qquad

  \inferrule*[lab=S-Ctxt]
  {
  e = \ctxt \cdot e_0\\
  \seval{\env}{m}{e_0}{\env'}{e_0'}\\
  e' = \ctxt \cdot e_0'
  }
  {
  \seval{\env}{m}{e}{\env'}{e'}
  }
\end{array}
$$
  \[\begin{array}{rlcl}
      \text{Eval Context} & \ctxt & ::= & \hole \mid \ewrite{\ctxt} \mid
                                          \epair{\ctxt}{e} \mid
                             \epair{v}{\ctxt} \mid \eproj{i}{\ctxt}  \\
               && \mid & \eshare{\psi}{P}{Q}{\ctxt} \mid
                             \ereveal{P}{Q}{\ctxt} \mid \ebinop{\ctxt}{e_2} \mid \ebinop{v}{\ctxt}   \\
               && \mid & \emux{\ctxt}{e_1}{e_2}
                         \mid \emux{v}{\ctxt}{e_2} \mid \emux{v}{v_1}{\ctxt}    \\
               && \mid & \einj{i}{\ctxt} \mid \econd{\ctxt}{e_1}{e_2}   \\
               && \mid & \efold{\trec{\alpha}{\tau}}{\ctxt} \mid \eunfold{\ctxt}  \\
                          && \mid & \eapp{\ctxt}{e} \mid
                         \eapp{v}{\ctxt} \mid \elet{x}{\ctxt}{e_2} \\
    \end{array}
  \]
\caption{Operational Semantics (Single threaded, small step)}
\label{fig:small-sem}
\end{figure}

\section{Semantics (Multi-threaded)}
\label{sec:MTsemantics}

We have not developed a multi-threaded semantics as yet, but we have
one in mind. Here's how it will work.

Each party $A$ has its own store $\env_A$ and program $e_A$, rather
than there being a global program $e$. These programs are the same as
$e$ except they start out as wrapped in a $\epar{\{A\}}{...}$ for each
principal $A$ that is participating in the MPC\@. To be concrete:
Suppose you are given a program $e$
meant to be run by both $A$ and $B$. At host $A$ you just run
$\epar{\{A\}}{e}$ and at $B$ you run $\epar{\{B\}}{e}$; i.e., you wrap
the program $e$ with a $\kw{par}$ annotation of the host running
it. Doing will have the effect of intersecting the set $P$ of any
$\epar{P}{...}$ within $e$ that is reached by $A$ or $B$, singly. If
doing so causes the $\kw{par}$ expression's principal set to be empty,
the expression will be skipped (and a $\vcrash$ value
returned). Otherwise it will be run by that single principal.

By inspection of the evaluation rules (Figure~\ref{fig:small-sem}), we can
see that programs without $\kw{share}$ and $\kw{reveal}$ operations
should work out fine---if the program would have run correctly as a
single-threaded program, it should run correctly in this
multi-threaded manner, too. The trick is that programs that don't do
$\kw{share}$ and $\kw{reveal}$ don't communicate between
principals. Those that do require coordination, so they won't work in
the above approach. For example, if we had
$\eshare{\psi}{\{A\}}{\{A,B\}}{e}$, then rule \rulelab{E-Share} will
get stuck: the annotation $\{A,B\}$ requires both $A$ and $B$ to be
present, but only one of them will be when running in
$\epar{\{A\}}{...}$ or $\epar{\{B\}}{...}$ (which would also get stuck
because the annotation $A$ would be invalid in mode $\{B\}$).

To support MPCs, we need to introduce additional rules that allow
single principals that reach MPC constructs to make progress as long
as they all make progress. That is, the single-mode parties must
synchronize before carrying out the computation. They will wait for
each party to reach the same spot, carry out the computation, and
continue on with whatever their local result should be. The type
system should ensure things line up.

I imagine a MT judgment of the form
$\mathcal{CS} \longrightarrow \mathcal{CS'}$ where $\mathcal{CS}$
represents a set of configurations, one per active principal. Here's
the basic way a principal takes a step:
$$
\begin{array}{c}
    \inferrule*[lab=M-Step]
    {
  \mathcal{CS} = \mathcal{CS}_0 :: (p,\config{\env_A}{e_A}) ::
  \mathcal{CS}_1\\
  \seval{\env_A}{\{A\}}{e_A}{\env_A'}{e'_A}\\
  \mathcal{CS}' = \mathcal{CS}_0 :: (p,\config{\env_A'}{e_A'}) ::
  \mathcal{CS}_1\\
  }
  {
  \mathcal{CS} \longrightarrow \mathcal{CS'}
  }
\end{array}
$$
The issue is what happens when principals need to perform a
coordinated MPC\@. Here's the form of the rule I have in mind for
shares, specialized to two parties $A$ and $B$:
$$
\begin{array}{c}
    \inferrule*[lab=M-Share]
    {
  \mathcal{CS} = \mathcal{CS}_0 ::
  (p,\config{\env_A}{e_A}) ::
  (q,\config{\env_B}{e_B}) ::
  \mathcal{CS}_1\\\\
  e_A = \pctxt_A \cdot \eshare{\psi}{\{A\}}{\{A,B\}}{v_A}\\\\
  e_B = \pctxt_B \cdot \eshare{\psi}{\{A\}}{\{A,B\}}{v_B}\\\\
  \mathit{mode}(\pctxt) = \mathit{mode}(\pctxt') = \{A,B\}\\\\
  \getat{\{A\}}{v_A} = n\\\\
  e_A' = \pctxt_A \cdot \vloc{\vshare{\{A,B\}}{\psi}{n}}{\{A\}}\\\\
  e_B' = \pctxt_B \cdot \vloc{\vshare{\{A,B\}}{\psi}{n}}{\{B\}}\\\\
  \mathcal{CS}' = \mathcal{CS}_0 ::
  (p,\config{\env_A}{e_A'}) ::
  (q,\config{\env_B}{e_B'}) ::
  \mathcal{CS}_1\\
  }
  {
  \mathcal{CS} \longrightarrow \mathcal{CS'}
  }
\end{array}
$$
Here, $\pctxt_A$ and $\pctxt_B$ are composition of contexts
of the following form:
$$
\begin{array}{lcl}
  \multicolumn{3}{l}{\pctxt \; ::= \; \ctxt \cdot (\epar{Q}{\pctxt})\;
  \mid\; \ctxt}\\
\end{array}
$$
The function $\mathit{mode}(\pctxt)$ returns the mode under which the
expression plugged into $\hole$ will be evaluated.
$$
\begin{array}{lcl}
  \mathit{mode}(\ctxt) & = & \emptyset\\
  \mathit{mode}(\ctxt \cdot (\epar{Q}{\pctxt})) & = & Q \cap \mathit{mode}(\pctxt)\\
\end{array}
$$

The rule requires both $A$ and $B$ to arrive at a redex suitable
for sharing---the mode of both evaluation contexts is
$\{A,B\}$. Since $A$ is doing the sharing, its value should be an
(accessible) integer $n$; the value $v_B$ at $B$ is immaterial
(probably something inaccessible, since it was computed only at $A$).
The rule takes a step by communicating the share
value $\vshare{\{A,B\}}{\psi}{n}$ to each party $A$ and $B$ (and the
share is specifically located there).

We likewise coordinate progress on shared values on the rules
equivalent to \rulelab{E-BinopShare} and \rulelab{E-Mux}, but for
these, both parties should take identical steps on identical share
values. We also coordinate on rules equivalent to
\rulelab{E-RevealPlain} and \rulelab{E-Conv}
(where each party has the identical share value again), sending the
unencrypted/converted result to the desired party. It's possible we'll want to
revisit the ``suspended'' computation of shared values and just
compute them more eagerly.

That coordination is always possible is guaranteed by the type system,
and the simulation theorem. To prove this, we'll need some kind of
``slicing'' judgment that splits up the environment $\env$ for the ST
case into the smaller environments in the MT case. Put another way,
environments may disagree on particular values, e.g., party $A$'s
environment $\env_A$ might map $x$ to $\vcrash$ while party $B$'s
environment $\env_B$ maps it to $\vloc{n}{\{B\}}$. We may also need to
define a notion of party-equivalence for contexts $\ctxt$, so that we
can properly match up two parties' programs that have reached the same
point. If we are lucky, these differences will all be localized in
environments, and contexts can be the same. See the Wysteria paper for
approach and inspiration!

% More details:
% \begin{itemize}
% \item $\eshare{P}{\psi}{Q}{e}$ requires coordination among $P \cup Q$,
%   where $P$ is a singleton. Somehow, the result of this computation
%   will be $\vshare{Q}{\psi}{n}$ and given to each party $q \in Q$,
% whereas $P$ gets $0$ (if he is not in $Q$).
% \item $\ebinop{e_1}{e_2}$ is just as in the ST mode---each party in
%   the share will have the full copy of it. Same with $\kw{mux}$. Key
%   question: How to prove these end up in the same place?
% \item $\ereveal{P}{\psi}{Q}{e}$ requires coordination among $P \cup
%   Q$, where $e$ is a $\vshare{Q}{\psi}{e'}$---i.e., the $Q$ parties
% match up. Each of the parties $q \in Q$ should reach this redex
% \emph{and have the same $e'$}. The type system should ensure they
% agree on this, the circuit that's been created. Moreover, $e'$ should
% only consist of numbers, binops, and muxes. This means you can just
% \emph{run it without an environment, in an empty mode}. We let every
% $q \in Q$ do that, and thus they will agree on the result $n$. Parties
% $p \in P$ then receive this result, while any other parties $q \not\in
% P$ receive $0$.
% \end{itemize}

% To prove that this semantics matches the ST one, I think we need some
% kind of ``slice'' of a program, for each of the involved hosts. At
% present, I think this slice can be static---you take the program,
% create a slice of it, and then run each slice. When we have dependent
% types, that may not be true anymore.

\section{Type System}
\label{sec:typing}

\begin{figure}
\[\begin{array}{c}

    \inferrule*[lab=T-Nat]
    {
    }
    {
    \hastyp{\Gamma}{m}{n}{\tnat^{m}}
    } \qquad

    \inferrule*[lab=T-Var]
    {
    x\!:\!\tau' \in \Gamma\\\\
    \issub{\tau'}{\tau}\\ m \vdash \tau
    }
    {
    \hastyp{\Gamma}{m}{x}{\tau}
    }\qquad

    \inferrule*[lab=T-Read]
    {
    m\text{ is a singleton}
    }
    {
    \hastyp{\Gamma}{m}{\eread}{\tnat^{m}}
    }
    \qquad

    \inferrule*[lab=T-Write]
    {
    m\text{ is a singleton}\\\\
    \hastyp{\Gamma}{m}{e}{\tnat^{m}}
    }
    {
    \hastyp{\Gamma}{m}{\ewrite{e}}{\tnat^{m}}
    }
    \\ \\
    
    \inferrule*[lab=T-Par]
    {
    Q = m \cap P\\
    Q \not= \emptyset\\\\
    \hastyp{\Gamma}{{Q}}{e}{\tau}
    }
    {
    \hastyp{\Gamma}{m}{\epar{P}{e}}{\tau}
    } \qquad

    \inferrule*[lab=T-ParEmp]
    {
    m \cap P = \emptyset\\\\
    \emptyset \vdash \tau
    }
    {
    \hastyp{\Gamma}{m}{\epar{P}{e}}{\tau}
    } \qquad

    \inferrule*[lab=T-Pair]
    {
    \hastyp{\Gamma}{m}{e_1}{\tau_1}\\\\
    \hastyp{\Gamma}{m}{e_2}{\tau_2}
    }
    {
    \hastyp{\Gamma}{m}{\epair{e_1}{e_2}}{\tau_1 \times \tau_2}
    }\qquad
    
    \inferrule*[lab=T-Access]
    {
    \hastyp{\Gamma}{m}{e}{\tau_1 \times \tau_2}\\
    }
    {
    \hastyp{\Gamma}{m}{\eproj{i}{e}}{\tau_i}
    }\\\\

    \inferrule*[lab=T-inj]
    {
    \hastyp{\Gamma}{m}{e}{\tau}
    }
    {
    \hastyp{\Gamma}{m}{\einj{1}{e}}{\tsum{\tau}{\tau_0}{m}}\\\\
    \hastyp{\Gamma}{m}{\einj{2}{e}}{\tsum{\tau_0}{\tau}{m}}
    } \qquad
    
    \inferrule*[lab=T-Match]
    {
    \hastyp{\Gamma}{m}{e}{\tau_0 +^m \tau_1}\\\\
    m \dashv \tau_0 \\
    m \dashv \tau_1 \\\\
    \hastyp{\Gamma,x\!:\!\tau_0}{m}{e_1}{\tau}\\
    \hastyp{\Gamma,x\!:\!\tau_1}{m}{e_2}{\tau}\\
    }
    {
      \hastyp{\Gamma}{m}{\econd{e}{e_1}{e_2}}{\tau}
    }\\ \\

    \inferrule*[lab=T-Fold]
    {
    \hastyp{\Gamma}{m}{e}{\tau[\trec{\alpha}{\tau}\setminus\alpha]}
    }
    {
    \hastyp{\Gamma}{m}{\efold{\trec{\alpha}{\tau}}{e}}{\trec{\alpha}{\tau}}
    } \qquad

    \inferrule*[lab=T-Unfold]
    {
    \hastyp{\Gamma}{m}{e}{\trec{\alpha}{\tau}}
    }
    {
    \hastyp{\Gamma}{m}{\eunfold{e}}{\tau[\trec{\alpha}{\tau}\setminus\alpha]}
    } \qquad

    \inferrule*[lab=T-Share]
    {
    P\text{ is a singleton}    \\
    m \vdash P\\\\
    \hastyp{\Gamma}{m}{e}{\tnat^P}\\
    m \vdash Q\\
    }
    {
    \hastyp{\Gamma}{m}{\eshare{\psi}{P}{Q}{e}}{\tnat^Q_\psi}
    }\\ \\
    
    \inferrule*[lab=T-Binop]
    {
    \tau = \tnat^m_\psi\\\\
    \hastyp{\Gamma}{m}{e_1}{\tau}\\\\
    \hastyp{\Gamma}{m}{e_2}{\tau}\\
    }
    {
    \hastyp{\Gamma}{m}{\ebinop{e_1}{e_2}}{\tau}
    }\qquad

    \inferrule*[lab=T-Mux]
    {
    \tau = \tnat^m_\psi\\\\
    \hastyp{\Gamma}{m}{e}{\tau}\\\\
    \hastyp{\Gamma}{m}{e_1}{\tau}\\
    \hastyp{\Gamma}{m}{e_2}{\tau}\\
    }
    {
    \hastyp{\Gamma}{m}{\emux{e}{e_1}{e_2}}{\tau}
    }\qquad

    \inferrule*[lab=T-Reveal]
    {
    m \vdash P\\
    m \vdash Q\\\\
    \hastyp{\Gamma}{m}{e}{\tnat^P_{\psi}}\\
    }
    {
    \hastyp{\Gamma}{m}{\ereveal{\psi'}{Q}{e}}{\tnat^Q_{\psi'}}
    }\\ \\
   

   \inferrule*[lab=T-Abs]
    {
    \hastyp{\Gamma, x\!:\!\tau_1}{m}{e}{\tau_2}
    }
    {
    \hastyp{\Gamma}{m}{\elam{x}{e}}{\tfun{\tau_1}{\tau_2}{m}}
    }\qquad
   
    \inferrule*[lab=T-App]
    {
    \hastyp{\Gamma}{m}{e}{\tfun{\tau_1}{\tau}{m}}\\\\
    \hastyp{\Gamma}{m}{e_1}{\tau_1}\\
    }
    {
    \hastyp{\Gamma}{m}{\eapp{e}{e_1}}{\tau}
    }\qquad

    \inferrule*[lab=T-Fix]
    {
    \hastyp{\Gamma,f\!:\! \tfun{\tau_1}{\tau_2}{m}}{m}{\elam{x}{e}}{\tfun{\tau_1}{\tau_2}{m}}
    }
    {
    \hastyp{\Gamma}{m}{\efix{f}{x}{e}}{\tfun{\tau_1}{\tau_2}{m}}
    }\\ \\

    \inferrule*[lab=T-Let]
    {
    \hastyp{\Gamma}{m}{e_1}{\tau_1}\\\\
    \hastyp{\Gamma, x\!:\!\tau_1}{m}{e_2}{\tau}
    }
    {
    \hastyp{\Gamma}{m}{\elet{x}{e_1}{e_2}}{\tau}
    }\qquad

    \inferrule*[lab=T-Sub]
    {
    \hastyp{\Gamma}{m}{e}{\tau_1}\\
    \issub{\tau_1}{\tau_2}
    }
    {
    \hastyp{\Gamma}{m}{e}{\tau_2}
    }

  \end{array}
\]
\caption{Typing rules}
\label{fig:typing}
\end{figure}

While PSL is not statically typed at present, the plan is to make it
so, eventually. As a preliminary step, we have developed a type system
for \lang, but it has some deficiencies that will require further work.

The typing judgment's rules are defined in Figure~\ref{fig:typing}.
The typing judgment has form $\hastyp{\Gamma}{m}{e}{\tau}$. It states
that under context $\Gamma$, expression $e$ has type $\tau$ when
evaluating in mode $m$. The rules reference judgment $m \vdash m'$,
discussed earlier, in Figure~\ref{fig:aux}. They also reference
judgment $m \vdash \tau$, which says that $\tau$ is located at 
one or more principals in $m$; and $m \dashv \tau$, which says it is
located at \emph{all} principals in $m$. 

Rule~\rulelab{T-Nat} types a (cleartext) constant; it inherits the
visibility of the current mode. Rule~\rulelab{T-Read} types the
\kw{read} command; since it will produce a different value at each
location, it must be run in the visibility of a single principal ($m$
must be a singleton set). Rule~\rulelab{T-Write} is
similar.

Rule~\rulelab{T-Var} ensures that the variable it looks up is given a
type compatible with the current mode $m$. It looks up the type
$\tau'$ of $x$ in the environment, and then invokes subtyping
$\issub{\tau'}{\tau}$ in an attempt to find a type $\tau$ compatible
with mode $m$. Compatibility $m \vdash \tau$, given in
Figure~\ref{fig:aux}, holds if the classified-as-$\tau$ value is
``located'' at least some principals $B \in m$. The subtyping judgment
is also given in Figure~\ref{fig:sub}. Intuitively,
$\issub{\tau_1}{\tau_2}$ holds when $\tau_1$ is compatible with the
maximal mode that $\tau_2$ is compatible with; i.e., it's OK to treat
a value as being available at fewer locations than it actually is. The
combination of compatibility and subtyping correspond to the semantic
function $\locof{m}{v}$, which locates $v$ at principals in $m$
(filtering out principals not present there). Rule~\rulelab{T-Sub} is
used to invoke subtyping directly.

Rule~\rulelab{T-Par} types the expression $e$ in par mode involving
principals $P$. At run-time, only $m \cap P$ principals will actually
execute $e$, and so $e$ is checked in this mode, which we name
$Q$.\footnote{Wysteria requires $P \subseteq m$, which is always OK in
  \rulelab{T-Par}, too, but it's strictly more flexible.}
Rule~\rulelab{T-ParEmp} handles the case that $m \cap P = \emptyset$
so the expression $e$ is not actually executed. As such, it will
return $\vcrash$, which we can type as having any type $\tau$ that is
compatible with $\emptyset$.

Rule~\rulelab{T-Pair} types the introduction of pairs. A pair's
components must be typeable in the current mode, but they do not
necessarily need to have the same visibility; e.g., in mode $\{A,B\}$
a pair could have one component at $A$ and the other at
$B$. Rule~\rulelab{T-Access} types the elimination of pairs. This rule
is standard.

Rules~\rulelab{T-Inj} is standard. Rule~\rulelab{T-Match} requires
that both of the to-be-accessed types are \emph{located} at all of the
principals in the current mode, per the judgment $m \dashv \tau$. This
judgment is a sort of dual to $m \vdash \tau$; it requires that all
components of $\tau$ are present to principals in $m$. We can think of
this judgment as being related to semantic function $\getat{m}{v}$: If $v$
has type $\tau$ and $m \dashv \tau$ (with the expected extensions to
typing for new value forms), then $\getat{m}{v}$ should succeed.

The rules \rulelab{T-Fold} and \rulelab{T-Unfold} for recursive types
are standard. Rule \rulelab{T-Let} (at the bottom) is, too.

Rule~\rulelab{T-Share} types encrypting a $\tnat$ (via secret
sharing). The $P$ argument indicates the principal doing the sharing,
and it must be a singleton. The $Q$ argument indicates the principals
to which to share $e$ (which must be a normal (non-share) value). This
value of $e$ must be visible in the current mode ($m \vdash P$) and
the principals $Q$ must be present as well. Finally, the value must be
a number; neither pairs nor functions can be shares.

Rule~\rulelab{T-Binop} types arithmetic computations on both shares
and normal values---both arguments must have the same type (i.e., both
shares or both normal values, with the same visibility), and match the
current mode. Note that this rule precludes adding a 
share and a normal value; you can always do this by converting the
latter to a share (the compiler can be smart about this).

Rule~\rulelab{T-Mux} types multiplexing. The semantics is to evaluate
both branches (second and third arguments), binding $x$ to the left
and right-hand sides of the sum, respectively. The mux chooses
the result to return based on the first argument's ultimate
result. These are all (compatible) shares (under protocol $\psi$), so
we can think of this as making a multiplexor circuit. The rule
restrict the results to be $\tnat$s, but this is easily generalized
via encoding (Section~\ref{sec:generalmux}).

Rule~\rulelab{T-Reveal} types share elimination, i.e., converting a
share of a $\tnat$ to a normal value, or to another share type. The
rule requires that the current mode includes those in $Q$, and those
$P$ who hold the shares.

Rule~\rulelab{T-Match} types conditionals on normal sums, i.e., the
conditional will run on each principal in mode $m$. It can return
shares or normal values, as desired. 

\rulelab{T-Abs} types the body of a function in the defining context's
mode $m$. \rulelab{T-App} requires the caller's mode to match the mode
annotation on the function; this ensures that all principals that must
be present in the function body will indeed be running the
function. \rulelab{T-Fix} is essentially a combination of these two,
allowing $f$ to be referred to recursively in $e$.

% Note that when we develop a small-step operational semantics we'll
% have to develop type rules for special value forms too. Crash values
% $\vcrash$ can be given any type that is compatible with mode
% $\emptyset$. Located values $\vloc{u}{P}$ will have the type of $u$
% annotated with location $P$.

\mwh{TODO: How are curried functions located: how is \texttt{m} on the
  arrow handled? No subtyping at present for recursive types. Should
  we add it?} 

\begin{figure}
\[\begin{array}{c}

    \inferrule*[lab=M-sub]
    {
    P \subseteq Q
    }
    {
    Q \vdash P
    } \qquad
    
    \inferrule*[lab=M-Nat]
    {
    m = m' \vee
    (\psi = \cdot \Rightarrow m \vdash m')
    }
    {
    m \vdash \tnat^{m'}_\psi
    } \qquad

    \inferrule*[lab=M-Sum]
    {
    m \vdash m'\\
    m \vdash \tau_1 \\ m \vdash \tau_2
    }
    {
    m \vdash \tsum{\tau_1}{\tau_2}{m'}
    } \\ \\

    \inferrule*[lab=M-Fun]
    {
    }
    {
    m \vdash \tfun{\tau_1}{\tau_2}{m}
    }\qquad

    \inferrule*[lab=M-Prod]
    {
    m \vdash \tau_1 \\ m \vdash \tau_2
    }
    {m \vdash \tpair{\tau_1}{\tau_2}}
    \qquad

    \inferrule*[lab=M-Rec]
    {m \vdash \tau}
    {m \vdash \trec{\alpha}{\tau}}
    \qquad
    
    \inferrule*[lab=m-Alpha]
    { }
    {m \vdash \alpha}
    \\ \\

    \inferrule*[lab=L-Nat]
    {  }
    {
    m \dashv \tnat^m_\psi
    } \qquad

    \inferrule*[lab=L-Sum]
    {
    m \dashv \tau_1 \\\\ m \dashv \tau_2
    }
    {
    m \dashv \tsum{\tau_1}{\tau_2}{m}
    } \qquad

    \inferrule*[lab=L-Fun]
    {
    }
    {
    m \dashv \tfun{\tau_1}{\tau_2}{m}
    }\qquad

    \inferrule*[lab=L-Prod]
    {
    m \dashv \tau_1 \\\\ m \dashv \tau_2
    }
    {m \dashv \tpair{\tau_1}{\tau_2}}
    \qquad

    \inferrule*[lab=L-Rec]
    {m \dashv \tau}
    {m \dashv \trec{\alpha}{\tau}}
    \qquad
    
    \inferrule*[lab=L-Alpha]
    { }
    {m \dashv \alpha}
    \\ \\
    
  \end{array}\]
\caption{Locatedness and Presence}
\label{fig:aux}
\end{figure}
    
\begin{figure}
\[\begin{array}{c}

    \inferrule*[lab=S-Nat]
    {
    m \vdash \tnat^{m'}_\psi
    }
    {
    \issub{\tnat^m_\psi}{\tnat^{m'}_\psi}
    } \qquad

    \inferrule*[lab=S-Sum]
    {
    m \vdash \tsum{\tau_1'}{\tau_2'}{m'}\\\\
    \issub{\tau_1}{\tau_1'} \land \issub{\tau_2}{\tau_2'}
    }
    {
    \issub{\tsum{\tau_1}{\tau_2}{m}}{\tsum{\tau_1'}{\tau_2'}{m'}}
    } \qquad
    
    \inferrule*[lab=S-Fun]
    {
    \issub{\tau_1'}{\tau_1}\\
    \issub{\tau_2}{\tau_2'}
    }
    {
    \issub{\tfun{\tau_1}{\tau_2}{m}}{\tfun{\tau_1'}{\tau_2'}{m}}
    }\qquad

    
    \inferrule*[lab=S-Pair]
    {
    \issub{\tau_1}{\tau_1'}\\
    \issub{\tau_2}{\tau_2'}
    }
    {
    \issub{\tau_1 \times \tau_2}{\tau_1' \times \tau_2'}
    }
    
\end{array}
\]
\caption{Subtyping}
\label{fig:sub}
\end{figure}

\subsection{Discussion: Polymorphism and Function Types}

The rules for functions are limited by the lack of polymorphism in
\lang's type system. Consider the following example:
\begin{verbatim}
par{A,B}
  let f = fun x -> par{A} (x,1) in
  let y = f (par{A,B} 1) in 
  let y' = f (par{A} 1) in ...
\end{verbatim}
The function argument $x$'s type is forced by the type system to be
monomorphic. To make both calls succeed, we could give the function
the type
$\tfun{\tnat^{\{A\}}}{\tpair{\tnat^{\{A\}}}{\tnat^{\{A\}}}}{\{A,B\}}$. Then
the first call would use subtyping on the argument, since
$\issub{\tnat^{\{A,B\}}}{\tnat^{\{A\}}}$. However, what would happen
if added the line
\begin{verbatim}
let z = par{A} f 1 in ...
\end{verbatim}
This does not seem any different than the call assigning to
\texttt{y'} and yet the type system will reject it---we cannot call
\texttt{f} in a mode other than $\{A,B\}$. While for this function
calling it in mode $\{A\}$ would be safe, if the function's body
involved a $\kw{share}$ operation including both $A$ and $B$ (e.g., on
its argument $x$, it would have the same type but would not be safe to
call it just $A$.

We could imagine fixing this by trying to distinguish a lower bound
versus an upper bound on which principals should be present. For example,
if we had the function:
\begin{verbatim}
let f = fun () ->
  let y = par{A,B} share{A}{A,B} 0 in
  par{C} 0
\end{verbatim}
Function \texttt{f} must be called in a mode $m \supseteq \{A,B\}$ or
else $\kw{share}$ call will fail. But the return type will differ
depending on the actual choice of $m$. For example, if we call
\texttt{f} in mode $\{A,B\}$ or $\{A,B,D\}$, the return type should be
$\tnat^\emptyset$. If we call \texttt{f} in mode $\{A,B,C\}$ or
$\{A,B,C,D\}$, , the return should be $\tnat^{\{C\}}$.

This leads naturally to the idea that mode indicators $m$ on types can
be \emph{polymorphic} (universally quantified), along with constraints
on their possible solutions. The above example could be given the type
$$
\forall \beta, \gamma. [\beta \supseteq \{A,B\}, \gamma = \beta \cap
\{C\}] \Rightarrow \tfun{\kw{unit}}{\tnat^\gamma}{\beta}
$$
Such a type conveys the minimum constraints on the caller's mode while
at the same time affords precise mode annotations on types affected by
the particular choice of mode.

To develop such an approach, we will need to have a logic for deciding
set constraints. We also need to adjust rules like \rulelab{T-Binop}
so that premises like $\tau = \tnat^m_\psi$ are generalized to
$\tau = \tnat^{m'}_\psi$ where $m' \vdash m$ and $m \vdash m'$. More
to do!

As discussed in Section~\ref{sec:deptypes}, we also want to support
first-class principal sets and dependent types including these. These
should be a natural addition to the approach we'll have to adopt for
normal polymorphism.

\subsection{Metatheory}

We conjecture two theoretical results, which we aim to prove.

\paragraph{Type soundness} If a program is type
correct, then it will either diverge or else run to completion in the
single-threaded semantics. More formally: If $\hastyp{}{m}{e}{\tau}$
then there exists a some $e_v$ such that $\eval{[]}{m}{e}{e_v}$, or
else for all $n$, $e$ can take $n$ steps in the small-step
semantics. This theorem establishes that the type system ensures the
program does not get ``stuck,'' e.g., by accessing a value on a host
that has no access to it, treating a share as if it were a function,
etc.

\paragraph{Simulation} For all type correct programs, the
multi-threaded semantics simulates the single-threaded one. To
formalize this, we need to define some auxiliary functions that relate
a ST configuration $\config{\env}{e}$ to the equivalent MT one,
$\mathcal{CS}$. Then we can prove that if $\mathcal{CS}$ can a take a
step, then $\config{\env}{e}$ can take a step too, and $\mathcal{CS}$
will eventually reach the point where the auxiliary function matches
them up. This theorem ensures that ST can be used to reason about
real-world MT executions.

\section{Conclusion and Future Work}

\mwh{TODO}

\end{document}
